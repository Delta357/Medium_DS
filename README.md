# Medium_Data Science

[![MIT License](https://img.shields.io/apm/l/atomic-design-ui.svg?)](https://github.com/tterb/atomic-design-ui/blob/master/LICENSEs)
[![GPLv3 License](https://img.shields.io/badge/License-GPL%20v3-yellow.svg)](https://opensource.org/licenses/)
[![AGPL License](https://img.shields.io/badge/license-AGPL-blue.svg)](http://www.gnu.org/licenses/agpl-3.0)
[![author](https://img.shields.io/badge/author-RafaelGallo-red.svg)](https://github.com/RafaelGallo?tab=repositories) 
[![](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-374/) 
[![](https://img.shields.io/badge/R-3.6.0-red.svg)](https://www.r-project.org/)
[![](https://img.shields.io/badge/ggplot2-white.svg)](https://ggplot2.tidyverse.org/)
[![](https://img.shields.io/badge/dplyr-blue.svg)](https://dplyr.tidyverse.org/)
[![](https://img.shields.io/badge/readr-green.svg)](https://readr.tidyverse.org/)
[![](https://img.shields.io/badge/ggvis-black.svg)](https://ggvis.tidyverse.org/)
[![](https://img.shields.io/badge/Shiny-red.svg)](https://shiny.tidyverse.org/)
[![](https://img.shields.io/badge/plotly-green.svg)](https://plotly.com/)
[![](https://img.shields.io/badge/XGBoost-red.svg)](https://xgboost.readthedocs.io/en/stable/#)
[![](https://img.shields.io/badge/Tensorflow-orange.svg)](https://powerbi.microsoft.com/pt-br/)
[![](https://img.shields.io/badge/Keras-red.svg)](https://powerbi.microsoft.com/pt-br/)
[![](https://img.shields.io/badge/CUDA-gree.svg)](https://powerbi.microsoft.com/pt-br/)
[![](https://img.shields.io/badge/Caret-orange.svg)](https://caret.tidyverse.org/)
[![](https://img.shields.io/badge/Pandas-blue.svg)](https://pandas.pydata.org/) 
[![](https://img.shields.io/badge/Matplotlib-blue.svg)](https://matplotlib.org/)
[![](https://img.shields.io/badge/Seaborn-green.svg)](https://seaborn.pydata.org/)
[![](https://img.shields.io/badge/Matplotlib-orange.svg)](https://scikit-learn.org/stable/) 
[![](https://img.shields.io/badge/Scikit_Learn-green.svg)](https://scikit-learn.org/stable/)
[![](https://img.shields.io/badge/Numpy-white.svg)](https://numpy.org/)
[![](https://img.shields.io/badge/PowerBI-red.svg)](https://powerbi.microsoft.com/pt-br/)

![Logo](https://img.freepik.com/fotos-gratis/conceito-de-negocio-com-holografia-grafica_23-2149160931.jpg?w=740&t=st=1685640330~exp=1685640930~hmac=631211c941de570ccad3c7b54a88d71557ac6ec668da516d33b3850a7136ee97)

Bem-vindo o repositório abrangente de códigos, datasets relacionados a temas importantes em ciência de dados, como saúde, processamento de linguagem natural, detecção de fake news, modelos de classificação e análise de séries temporais. Aqui você encontrará uma variedade de recursos que podem ajudá-lo em suas pesquisas, projetos e aprendizado.

## Saúde
Código para análise de dados médicos e predição de doenças.
Conjuntos de dados de registros médicos e informações de saúde pública.
Artigos sobre análise de dados para diagnóstico médico e detecção precoce de doenças.

## Processamento linguagem natural
Implementações de algoritmos de PLN, como tokenização, lematização e análise de sentimentos.
Conjuntos de dados para treinamento e teste de modelos de PLN.
Artigos sobre técnicas avançadas de PLN, como processamento de texto em larga escala e tradução automática.

## Detecção notícias falsas
Códigos para detecção automatizada de notícias falsas.
Conjuntos de dados com exemplos de notícias verdadeiras e falsas.
Artigos que exploram métodos de detecção de fake news usando técnicas de aprendizado de máquina e processamento de linguagem natural.

## Machine learning - Classificação, regressão, cluster
Implementações de algoritmos de classificação, como árvores de decisão, regressão logística e redes neurais.
Conjuntos de dados para treinamento e avaliação de modelos de classificação.
Artigos que comparam diferentes abordagens de classificação e técnicas de seleção de recursos.

## Análise e previsão de séries temporais.
Conjuntos de dados com séries temporais de diferentes domínios.
Artigos sobre métodos avançados de análise de séries temporais, incluindo modelos autoregressivos, ARIMA e redes neurais recorrentes.
Esperamos que esse repositório seja útil para você explorar e aprofundar seus conhecimentos em ciência de dados. Fique à vontade para explorar os recursos disponíveis e utilize-os como ponto de partida para seus próprios projetos e estudos. Lembre-se de sempre citar as fontes adequadas ao utilizar qualquer material deste repositório.

# Conclusão
Lembramos que todos os códigos, artigos e datasets estão disponíveis para uso educacional e acadêmico, seguindo as devidas referências e direitos autorais aplicáveis. Encorajamos os usuários a contribuírem com novos materiais e compartilharem seus próprios códigos e projetos. Esperamos que este repositório seja um recurso valioso para sua jornada em ciência de dados e que você encontre informações relevantes e úteis para suas necessidades. Se tiver alguma dúvida ou precisar de assistência adicional, não hesite em entrar em contato. Boa exploração e sucesso em seus projetos de ciência de dados!

## Autores

- [@RafaelGallo](https://github.com/RafaelGallo)


## Licença

[MIT](https://choosealicense.com/licenses/mit/)


## Resultados - Dos modelos machine learning 

- Melhorar o suporte de navegadores

- Adicionar mais integrações


## Variáveis de Ambiente

Para rodar esse projeto, você vai precisar adicionar as seguintes variáveis de ambiente no seu .env

`API_KEY`

`ANOTHER_API_KEY`

## Variáveis de Ambiente

Para rodar esse projeto, você vai precisar adicionar as seguintes variáveis de ambiente no seu .env

Instalando a virtualenv

`pip install virtualenv`

Nova virtualenv

`virtualenv nome_virtualenv`

Ativando a virtualenv

`source nome_virtualenv/bin/activate` (Linux ou macOS)

`nome_virtualenv/Scripts/Activate` (Windows)

Retorno da env

`projeto_py source venv/bin/activate` 

Desativando a virtualenv

`(venv) deactivate` 

Instalando pacotes

`(venv) projeto_py pip install flask`

Instalando as bibliotecas

`pip freeze`

## Instalação 

```bash
  conda install pandas 
  conda install scikitlearn
  conda install numpy
  conda install scipy
  conda install matplotlib
  conda install keras
  conda install tensorflow-gpu==2.5.0

  python==3.6.4
  numpy==1.13.3
  scipy==1.0.0
  matplotlib==2.1.2
```
Instalação do Python É altamente recomendável usar o anaconda para instalar o python. Clique aqui para ir para a página de download do Anaconda https://www.anaconda.com/download. Certifique-se de baixar a versão Python 3.6. Se você estiver em uma máquina Windows: Abra o executável após a conclusão do download e siga as instruções. 

Assim que a instalação for concluída, abra o prompt do Anaconda no menu iniciar. Isso abrirá um terminal com o python ativado. Se você estiver em uma máquina Linux: Abra um terminal e navegue até o diretório onde o Anaconda foi baixado. 
Altere a permissão para o arquivo baixado para que ele possa ser executado. Portanto, se o nome do arquivo baixado for Anaconda3-5.1.0-Linux-x86_64.sh, use o seguinte comando: chmod a x Anaconda3-5.1.0-Linux-x86_64.sh.

Agora execute o script de instalação usando.


Depois de instalar o python, crie um novo ambiente python com todos os requisitos usando o seguinte comando

```bash
conda env create -f environment.yml
```
Após a configuração do novo ambiente, ative-o usando (windows)
```bash
activate "Nome do projeto"
```
ou se você estiver em uma máquina Linux
```bash
source "Nome do projeto" 
```
Agora que temos nosso ambiente Python todo configurado, podemos começar a trabalhar nas atribuições. Para fazer isso, navegue até o diretório onde as atribuições foram instaladas e inicie o notebook jupyter a partir do terminal usando o comando
```bash
jupyter notebook
```
## Stack utilizada

**Machine learning:** Python, R

**Framework:** Scikit-learn

**Análise de dados:** Python, R

## Uso/Exemplos - Modelo machine learning

# Instalação

Instalação das bibliotecas para esse projeto no python.

```bash
  conda install pandas 
  conda install scikitlearn
  conda install numpy
  conda install scipy
  conda install matplotlib
  conda install nltk

  python==3.6.4
  numpy==1.13.3
  scipy==1.0.0
  matplotlib==2.1.2
  nltk==3.6.7
```
Instalação do Python É altamente recomendável usar o anaconda para instalar o python. Clique aqui para ir para a página de download do Anaconda https://www.anaconda.com/download. Certifique-se de baixar a versão Python 3.6. Se você estiver em uma máquina Windows: Abra o executável após a conclusão do download e siga as instruções. 

Assim que a instalação for concluída, abra o prompt do Anaconda no menu iniciar. Isso abrirá um terminal com o python ativado. Se você estiver em uma máquina Linux: Abra um terminal e navegue até o diretório onde o Anaconda foi baixado. 
Altere a permissão para o arquivo baixado para que ele possa ser executado. Portanto, se o nome do arquivo baixado for Anaconda3-5.1.0-Linux-x86_64.sh, use o seguinte comando: chmod a x Anaconda3-5.1.0-Linux-x86_64.sh.

Agora execute o script de instalação usando.


Depois de instalar o python, crie um novo ambiente python com todos os requisitos usando o seguinte comando

```bash
conda env create -f environment.yml
```
Após a configuração do novo ambiente, ative-o usando (windows)
```bash
activate "Nome do projeto"
```
ou se você estiver em uma máquina Linux
```bash
source "Nome do projeto" 
```
Agora que temos nosso ambiente Python todo configurado, podemos começar a trabalhar nas atribuições. Para fazer isso, navegue até o diretório onde as atribuições foram instaladas e inicie o notebook jupyter a partir do terminal usando o comando
```bash
jupyter notebook
```
    
## Exemplo Modelo exemplo modelo bagging

```
# Importação das bibliotecas
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Carregando o conjunto de dados Iris
data = load_iris()
X = data.data
y = data.target

# Divisão dos dados em treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criação do classificador base (árvore de decisão)
base_classifier = DecisionTreeClassifier()

# Criação do classificador bagging com 10 estimadores (10 árvores de decisão)
bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10)

# Treinamento do modelo bagging
bagging_classifier.fit(X_train, y_train)

# Previsões do modelo bagging para os dados de teste
y_pred = bagging_classifier.predict(X_test)

# Avaliação da precisão do modelo
accuracy = accuracy_score(y_test, y_pred)
print("Acurácia do modelo de Bagging: {:.2f}".format(accuracy))

Exemplo Modelo exemplo modelo Boosting

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Carregando o conjunto de dados Iris
data = load_iris()
X = data.data
y = data.target

# Divisão dos dados em treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criação do classificador base (árvore de decisão)
base_classifier = DecisionTreeClassifier(max_depth=1)

# Criação do classificador AdaBoost com 50 estimadores
adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50)

# Treinamento do modelo AdaBoost
adaboost_classifier.fit(X_train, y_train)

# Previsões do modelo AdaBoost para os dados de teste
y_pred = adaboost_classifier.predict(X_test)

# Avaliação da precisão do modelo
accuracy = accuracy_score(y_test, y_pred)
print("Acurácia do modelo AdaBoost: {:.2f}".format(accuracy))


```

## Feedback

Se você tiver algum feedback, por favor nos deixe saber por meio de rafaelhenriquegallo@gmail.com.br


## Screenshots

![App Screenshot](https://via.placeholder.com/468x300?text=App+Screenshot+Here)


## Melhorias

Que melhorias você fez no seu código? Ex: refatorações, melhorias de performance, acessibilidade, etc


## Referência

 - [Awesome Readme Templates](https://awesomeopensource.com/project/elangosundar/awesome-README-templates)
 - [Awesome README](https://github.com/matiassingers/awesome-readme)
 - [How to write a Good readme](https://bulldogjob.com/news/449-how-to-write-a-good-readme-for-your-github-project)


## Licença

[MIT](https://choosealicense.com/licenses/mit/)


## Suporte

Para suporte, mande um email para rafaelhenriquegallo@gmail.com
